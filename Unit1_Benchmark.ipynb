{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8szxKN5HiNuQ"
      },
      "source": [
        "# Unit 1 Hands-on: Generative AI & NLP Fundamentals\n",
        "\n",
        "Welcome to your interactive guide to **Generative AI**. This notebook is designed to be a step-by-step tutorial, explaining not just *how* to code, but *why* we use these tools.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YJ9vsG-YiNuS"
      },
      "source": [
        "## 1. Introduction & Setup\n",
        "\n",
        "In this section, we will set up our environment. But first, let's understand the tools we are using.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rmiuxoKyiNuT"
      },
      "source": [
        "### What is Hugging Face?\n",
        "\n",
        "Hugging Face (https://huggingface.co/) is often called the \"GitHub of AI\". It is a massive repository where researchers and companies share their trained models, datasets, and demos.\n",
        "\n",
        "Instead of training a model from scratch (which costs millions of dollars), we can download models like GPT-2, BERT, or RoBERTa directly from Hugging Face and use them.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GyLGXBY1iNuT"
      },
      "source": [
        "### What is the `transformers` library?\n",
        "\n",
        "The `transformers` library is the bridge between the models on Hugging Face and your code. It provides APIs to easily download, load, and run state-of-the-art pretrained models.\n",
        "\n",
        "It supports framework interoperability, meaning you can often move between PyTorch, TensorFlow, and JAX.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fsxlaqjPiNuT"
      },
      "source": [
        "### What is `pipeline()`?\n",
        "\n",
        "The `pipeline()` function is the most powerful high-level tool in the library. It abstracts away the complex math and processing into three simple steps:\n",
        "\n",
        "1.  **Preprocessing**: Converts your raw text into numbers (Tokens & IDs) that the model can understand.\n",
        "2.  **Model Inference**: The model processes the numbers and outputs predictions (logits).\n",
        "3.  **Post-processing**: The raw predictions are converted back into human-readable text (labels, answers, summaries).\n",
        "\n",
        "With just one line, `pipeline('task-name')` handles all of this for you.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Bid1E0GiNuT"
      },
      "source": [
        "### Import Pipeline\n",
        "Let's import this powerful function.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch transformers tensorflow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7x4hGhxBi1Eq",
        "outputId": "a29212f3-743c-43df-ae36-03f798af756c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.0+cpu)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.6)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.12/dist-packages (2.19.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.3)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (25.12.19)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.7.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (5.29.5)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.0.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.76.0)\n",
            "Requirement already satisfied: tensorboard~=2.19.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.19.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.10.0)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.15.1)\n",
            "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.5.4)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (0.18.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2026.1.4)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.1.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zeVftr9XiNuU",
        "outputId": "a63ebbee-9981-4f08-a7fb-1cc2d7872023"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:torchao.kernel.intmm:Warning: Detected no triton, on systems without Triton certain kernels will not work\n"
          ]
        }
      ],
      "source": [
        "from transformers import pipeline, set_seed, GPT2Tokenizer\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lOCr3srxiNuV"
      },
      "source": [
        "### Import Utilities\n",
        "We also need `nltk` for some traditional NLP tasks and `os` for file handling.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "6vki0tmFiNuW"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import nltk\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fBIypblxiNuW"
      },
      "source": [
        "### Loading the Course Material\n",
        "We will define the path to our course text file (`unit 1.txt`).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "xM9Mpb3EiNuW"
      },
      "outputs": [],
      "source": [
        "file_path = \"/content/unit 1.txt\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ZFXExxMiNuW"
      },
      "source": [
        "Now we read the file. This text will be the 'Knowledge Base' for our tasks later.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3kBgAPZOiNuX",
        "outputId": "14bfab01-504c-4513-b215-aebf1a498aa9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File loaded successfully!\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        text = f.read()\n",
        "    print(\"File loaded successfully!\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: '{file_path}' not found.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p-7kd1FyiNuX"
      },
      "source": [
        "Let's look at the first 500 characters to make sure we have the right data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XmsVq7qniNuX",
        "outputId": "11950994-e7ce-4b32-bb1c-6267904aa6f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Data Preview ---\n",
            "Generative AI and Its Applications: A Foundational Briefing\n",
            "\n",
            "Executive Summary\n",
            "\n",
            "This document provides a comprehensive overview of Generative AI, synthesizing foundational concepts, technological underpinnings, and practical applications as outlined in the course materials from PES University. Generative AI represents a transformative subset of Artificial Intelligence focused on creating novel content, a capability primarily driven by the advent of Large Language Models (LLMs). The evolution of ...\n"
          ]
        }
      ],
      "source": [
        "print(\"--- Data Preview ---\")\n",
        "print(text[:500] + \"...\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SjvnhvAuiNuX"
      },
      "source": [
        "## 2. Generative AI: Dumb vs. Smart Models\n",
        "\n",
        "Generative AI creates new content (text, images, audio). But the quality depends heavily on the model's size and training.\n",
        "\n",
        "We will compare two models:\n",
        "1.  **`distilgpt2`**: A 'distilled' version. It is smaller, faster, and requires less memory, but it might be less coherent (a \"Dumb\" model for this comparison).\n",
        "2.  **`gpt2`**: The standard version (The \"Smart\" model, though still small by modern standards).\n",
        "\n",
        "**How to access a model?**\n",
        "1.  Go to Hugging Face Models page.\n",
        "2.  Search for a task (e.g., 'Text Generation').\n",
        "3.  Pick a model (e.g., `gpt2`).\n",
        "4.  Copy the model name.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L0Gknld5iNuX"
      },
      "source": [
        "### Step 1: Set a Seed\n",
        "\n",
        "A **seed value** is used to make random results **reproducible**. When we set a seed, the random number generator starts from the same point each time, which means it will produce the **same sequence of random values**.\n",
        "\n",
        "Try running the code multiple times using the **same seed value** and observe the output.\n",
        "\n",
        "Now, change the seed value and run the code again. This time, the output **will change** because a different seed creates a different sequence of random numbers.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "H6E4iDesiNuX"
      },
      "outputs": [],
      "source": [
        "set_seed(42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TRMabJYuiNuX"
      },
      "source": [
        "### Step 2: Define a Prompt\n",
        "Both models will complete this sentence.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "12CqVXIHiNuY"
      },
      "outputs": [],
      "source": [
        "prompt = \"Generative AI is a revolutionary technology that\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kFswZWvxiNuY"
      },
      "source": [
        "# Text Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cpCYq2L0iNuY",
        "outputId": "c958d0e1-20fb-45e3-c4cb-d9d3bf28141a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "If you want to use `BertLMHeadModel` as a standalone, add `is_decoder=True.`\n",
            "Device set to use cpu\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generative AI is a revolutionary technology that................................................................................................................................................................................................................................................................\n"
          ]
        }
      ],
      "source": [
        "bert_generator = pipeline('text-generation', model='bert-base-uncased')\n",
        "output_bert = bert_generator(prompt, max_length=50, num_return_sequences=1)\n",
        "print(output_bert[0]['generated_text'])\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "roberta_generator = pipeline('text-generation', model='roberta-base')\n",
        "output_roberta = roberta_generator(prompt, max_length=50, num_return_sequences=1)\n",
        "print(output_roberta[0]['generated_text'])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mya_79HVsf82",
        "outputId": "aa81af6e-2070-483e-bafb-0f871429ec34"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "If you want to use `RobertaLMHeadModel` as a standalone, add `is_decoder=True.`\n",
            "Device set to use cpu\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generative AI is a revolutionary technology that\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bart_generator = pipeline('text-generation', model='facebook/bart-base')\n",
        "output_bart = bart_generator(prompt, max_length=50, num_return_sequences=1)\n",
        "print(output_bart[0]['generated_text'])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tj_PF55KtXYd",
        "outputId": "1c91cfbe-63d2-403d-d0b7-efc19c0ee2bc"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BartForCausalLM were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['lm_head.weight', 'model.decoder.embed_tokens.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Device set to use cpu\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generative AI is a revolutionary technology that squatSpoilerOtherwise Drawn Shak Shak32 Shak Shak Shak denim df df dfPatrickSel religious au au Shak Shak dfSpoiler slipsino chuck chuck32 df df32 Mavericks32 df Walk Drawn Drawn ShakJan chuck df df Walk slips32 dfOtherwise df df Drawn3232Thor df df chuck df Cr Walk Drawn CrazeazeazeFrames Drawn df Drawn Drawn Drawn37232ino32spawn df df aisle banned3232 Drawn Drawnaxe Drawn Drawneller Drawn Drawn Alvin df Alvin Drawn Drawn df 361 Shak Drawn Drawn workload Drawn Drawn32 df32 dfSpoiler Drawn Drawn debuggeraze df spots df origins Drawn Drawn spots df df slips charged df dfSel Molecular df dfStatusFrames Drawn Drawn Walk Drawn Shak Drawntravel Drawn DrawnSel df Drawn spots spots df spots spots Walk DrawnOtherwise dfFrames Drawn Walk Beet df Beet Walk DrawnPost Drawn Alvin skysc df df Beet Beet Drawnload spots df debugger Alvin df df sure Drawn Drawn origins df Beet debugger dfPost finding df Drawn finding df df futures df spots floral df dfPostPost Drawn Drawnbreak Drawn Drawn rot Drawn DrawnFrames dfAlert df Beet Drawn restoration Drawnkas Drawn debuggerkas debugger df dfFramesdy Drawn Drawn Beet dfQU df debugger skysc df debugger restoration Drawndy Drawn debugger iconic WalkFrames dfkas debugger Beet Beet Beet debugger accompanies df df laugh df debuggerQU\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fill-Mask"
      ],
      "metadata": {
        "id": "QWgT7dSdvO-6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mask_filler = pipeline(\"fill-mask\", model=\"bert-base-uncased\")\n",
        "\n",
        "masked_sentence = \"The goal of Generative AI is to create new [MASK].\"\n",
        "preds = mask_filler(masked_sentence)\n",
        "\n",
        "for p in preds:\n",
        "    print(f\"{p['token_str']}: {p['score']:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IW2Uob6gyVGs",
        "outputId": "09102038-c520-4470-bf0f-a6454b8d3444"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "applications: 0.06\n",
            "ideas: 0.05\n",
            "problems: 0.05\n",
            "systems: 0.04\n",
            "information: 0.03\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mask_filler = pipeline(\"fill-mask\", model=\"roberta-base\")\n",
        "\n",
        "masked_sentence = \"The goal of Generative AI is to create new <mask>.\"\n",
        "preds = mask_filler(masked_sentence)\n",
        "\n",
        "for p in preds:\n",
        "    print(f\"{p['token_str']}: {p['score']:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZsXLGw9LyVvf",
        "outputId": "f18f3fb4-d8ba-41b7-9a0f-9c3a2afd317b"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " AI: 0.07\n",
            " agents: 0.06\n",
            " intelligence: 0.05\n",
            " applications: 0.04\n",
            " insights: 0.04\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mask_filler = pipeline(\"fill-mask\", model=\"facebook/bart-base\")\n",
        "\n",
        "masked_sentence = \"The goal of Generative AI is to create new <mask>.\"\n",
        "preds = mask_filler(masked_sentence)\n",
        "\n",
        "for p in preds:\n",
        "    print(f\"{p['token_str']}: {p['score']:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1arhhHdOyXIi",
        "outputId": "66c542cf-be21-4a81-a607-1ee8ea89c2cf"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " ways: 0.16\n",
            " AI: 0.10\n",
            " and: 0.05\n",
            " models: 0.04\n",
            ",: 0.03\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question Answering"
      ],
      "metadata": {
        "id": "IPXGT3wivqye"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "qa_pipeline = pipeline(\"question-answering\", model=\"bert-base-uncased\")\n",
        "\n",
        "questions = [\n",
        "    \"What is the fundamental innovation of the Transformer?\",\n",
        "    \"What are the risks of using Generative AI?\"\n",
        "]\n",
        "\n",
        "for q in questions:\n",
        "    res = qa_pipeline(question=q, context=text[:5000])\n",
        "    print(f\"\\nQ: {q}\")\n",
        "    print(f\"A: {res['answer']}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YS4RxoBzxV8q",
        "outputId": "c7fa4ebc-361d-4d3f-c5cb-078eb45a3e05"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Q: What is the fundamental innovation of the Transformer?\n",
            "A: not spam\"\n",
            "\n",
            "Q: What are the risks of using Generative AI?\n",
            "A: not spam\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "qa_pipeline = pipeline(\"question-answering\", model=\"roberta-base\")\n",
        "\n",
        "questions = [\n",
        "    \"What is the fundamental innovation of the Transformer?\",\n",
        "    \"What are the risks of using Generative AI?\"\n",
        "]\n",
        "\n",
        "for q in questions:\n",
        "    res = qa_pipeline(question=q, context=text[:5000])\n",
        "    print(f\"\\nQ: {q}\")\n",
        "    print(f\"A: {res['answer']}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fvBgEpKcxXzR",
        "outputId": "032a7a7d-6a94-4e23-9a50-6c2fc7e764fa"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaForQuestionAnswering were not initialized from the model checkpoint at roberta-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Q: What is the fundamental innovation of the Transformer?\n",
            "A: it\n",
            "\n",
            "Q: What are the risks of using Generative AI?\n",
            "A: it\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "qa_pipeline = pipeline(\"question-answering\", model=\"facebook/bart-base\")\n",
        "\n",
        "questions = [\n",
        "    \"What is the fundamental innovation of the Transformer?\",\n",
        "    \"What are the risks of using Generative AI?\"\n",
        "]\n",
        "\n",
        "for q in questions:\n",
        "    res = qa_pipeline(question=q, context=text[:5000])\n",
        "    print(f\"\\nQ: {q}\")\n",
        "    print(f\"A: {res['answer']}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VQAN_J7YxZXU",
        "outputId": "5f04e967-a6aa-463d-8abd-218323bbbda4"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BartForQuestionAnswering were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Q: What is the fundamental innovation of the Transformer?\n",
            "A: LLMs). The evolution of these models, from early\n",
            "\n",
            "Q: What are the risks of using Generative AI?\n",
            "A: LLMs). The evolution of these models, from early\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}